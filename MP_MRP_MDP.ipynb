{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MP/MRP/MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elements**\n",
    "* State set $S$\n",
    "* Action set $A$\n",
    "* Transition matirx $P$ following the Markov property: $P(X_{t+1}|X_t) = P(X_{t+1}|X_t,...,X_0)$\n",
    "    - Or $P_{ss'}^a = P(X_{t+1} = s'| X_t = s, A_t = a)$\n",
    "* Reward: \n",
    "    - 1) $R_s = \\mathbb{E}[R_{t+1}|S_t = s]$ (or $\\mathbb{E}[R_{t+1}|S_t = s, A_t = a]$)\n",
    "    - 2) $r(s',s)$, where $R_s = \\sum_{s' \\in S}r(s',s)p(s',s)$\n",
    "    - 3) $r(s)$ which is assigned to each state (and action)\n",
    "* Discount factor: $\\gamma$\n",
    "* Policy: $\\pi(a|s) = P[A_t = a | S_t = s]$<br>\n",
    "\n",
    "**Processes**\n",
    "* Markov Process = state set $S$ + transition probability matrix\n",
    "    - $<S, P>$ \n",
    "* Markov Reward Process  = MP + Reward + Discount factor\n",
    "    - $<S, P, R, \\gamma>$ \n",
    "    - Return: $G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum^{\\infty}_{i=t+1}\\gamma^{i-t-1} R_i$ \n",
    "    - Value: $v(s) = \\mathbb{E}[G_{t}|S_t = s]$\n",
    "* Markov Decision Process = MRP (enviroment uncertainty) + action (agent uncertainty)\n",
    "    - $<S, P, R, \\gamma, A>$ \n",
    "    - MDP + policy = MRP, $P_{ss'} = \\sum_{a \\in A} \\pi(a|s) P_{ss'}^a$, $R_{s} = \\sum_{a \\in A} \\pi(a|s) R_{s}^a$\n",
    "    - State-value function $v_{\\pi}(s) = \\mathbb{E}[G_{t}|S_t = s]$ (follow $\\pi$ after state $s$)\n",
    "    - Action-value function $q_{\\pi}(s,a) = \\mathbb{E}[G_{t}|S_t = s, A_t = a]$ (follow $\\pi$ after action $a$)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "**MRP**\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    v(s) & = \\mathbb{E}[G_{t}|S_t = s] \\\\\n",
    "         & = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1}|S_t = s] \\\\\n",
    "         & = \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1})|S_t = s] \\\\\n",
    "         & = R_s + \\mathbb{E}[\\gamma G_{t+1}|S_t = s] \\\\\n",
    "         & = R_s + \\gamma \\sum_{s' \\in S} P_{ss'}\\mathbb{E}[G_{t+1}|S_{t+1} = s']\\\\\n",
    "         & = R_s + \\gamma \\sum_{s' \\in S} P_{ss'}v(s')\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "**MDP**\n",
    "* Non-optimal v-q (1.1) \n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    v_{\\pi}(s) & = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Non-optimal q-v (1.2) \n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    q_{\\pi}(s,a) & = R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\pi}(s')\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Non-optimal v-v (1.3)\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    v_{\\pi}(s) & = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) \\\\\n",
    "    & = \\sum_{a \\in A} \\pi(a|s)(R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\pi}(s'))\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "or\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    v_{\\pi}(s) & = \\mathbb{E}_{\\pi}[G_{t}|S_t = s] \\\\\n",
    "    & = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t = s] \\\\\n",
    "    & = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_t = s] \n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Non-optimal q-q (1.4)\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    q_{\\pi}(s,a) & = R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\pi}(s')\\\\\n",
    "    & = R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a (\\sum_{a' \\in A} \\pi(a'|s')q_{\\pi}(s',a')) \n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "or\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    q_{\\pi}(s,a) & = \\mathbb{E}_{\\pi}[G_{t}|S_t = s, A_t = a] \\\\\n",
    "    & = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t = s, A_t = a] \\\\\n",
    "    & = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t = s, A_t = a] \n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Optimal v-q (2.1)\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    v_{\\ast}(s) & = \\max_{a} q_{\\ast}(s,a)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Optimal q-v (2.2)\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    q{\\ast}(s,a) & = \\max_{\\pi}{R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\pi}(s')}\\\\\n",
    "    & = R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\ast}(s')\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Optimal v-v (2.3)\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    v_{\\ast}(s) & = \\max_{a} q_{\\ast}(s,a) \\\\\n",
    "    & = \\max_{a} {R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\ast}(s')}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "* Optimal q-q (2.4)\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    q_{\\ast}(s,a) & = R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a v_{\\ast}(s')\\\\\n",
    "    & = R_s^{a} + \\gamma \\sum_{s \\in S'} P_{ss'}^a \\max_{a'} q_{\\ast}(s',a')\n",
    "\\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class design for MP\n",
    "**Write code to generate the stationary distribution for an MP** <br>\n",
    "We know that if $\\pi$ is the stationary distribution of an MP with transition matrix of A, then $\\pi = \\pi A$. <br>\n",
    "Equivalently, we find vector $\\pi$ as a left eigenvector of A with eigenvalue 1. Thus, finding the stationary distribution is equal to solving the eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "T = TypeVar(\"T\",str,int,float)\n",
    "\n",
    "# Identity helper function for str, int and float\n",
    "def ind(x: T, y: T):\n",
    "    if x == y or np.abs(x-y)<1e-5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Get state helper function\n",
    "def get_states_helper(in_graph: dict) -> dict:\n",
    "    state_list = list(in_graph.keys())\n",
    "    ind = range(len(state_list))\n",
    "    state = dict(zip(state_list,ind))\n",
    "    return state\n",
    "\n",
    "# Get transition matrix helper function\n",
    "def get_transition_helper(in_graph: dict) -> np.ndarray:\n",
    "    state = get_states_helper(in_graph)\n",
    "    tran_mat = np.zeros((len(state),len(state)))\n",
    "    for i, row in in_graph.items():\n",
    "        for j, prob in row.items():\n",
    "            ind_row = state[i]\n",
    "            ind_col = state[j]\n",
    "            if ind(tran_mat[ind_row,ind_col],0):\n",
    "                tran_mat[ind_row,ind_col] = prob\n",
    "    return tran_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sunny': 0, 'Cloudy': 1, 'Rainy': 2, 'Windy': 3}\n",
      "[[0.1  0.2  0.3  0.4 ]\n",
      " [0.25 0.25 0.3  0.2 ]\n",
      " [0.1  0.2  0.3  0.4 ]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# Test helper functions\n",
    "Input = {'Sunny': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Windy': 0.4},\n",
    "         'Cloudy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.3, 'Windy': 0.2},\n",
    "         'Rainy': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Windy': 0.4},\n",
    "         'Windy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.25, 'Windy': 0.25}}\n",
    "print(get_states_helper(Input))\n",
    "print(get_transition_helper(Input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MP by Graph\n",
    "\"\"\"\n",
    "    E.g.,\n",
    "    Input = {'Sunny': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Cloudy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.3, 'Cloudy': 0.2},\n",
    "             'Rainy': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Windy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.25, 'Cloudy': 0.25}}\n",
    "    Meaning: Today's weather => tmr's weather\n",
    "\"\"\"\n",
    "\n",
    "class MP:\n",
    "    # Initiate state dict & transition matrix\n",
    "    def __init__(self, in_graph: dict) -> None:\n",
    "        self.graph = in_graph\n",
    "        state = get_states_helper(in_graph)\n",
    "        tran_mat = get_transition_helper(in_graph)\n",
    "        # Check transition matrix and match state set with transition probs\n",
    "        if np.linalg.norm(np.sum(tran_mat, axis = 1)- np.ones(tran_mat.shape[0]))>1e-5:\n",
    "            raise ValueError\n",
    "        elif len(state) != tran_mat.shape[0]:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            self.state: dict = state\n",
    "            self.tran_mat: np.ndarray = tran_mat\n",
    "            \n",
    "    # Get all states\n",
    "    def get_states(self) -> set:\n",
    "        return self.state\n",
    "    \n",
    "    # Get the transition matirx\n",
    "    def get_tran_mat(self) -> np.ndarray:\n",
    "        return self.tran_mat\n",
    "    \n",
    "    # Compute stationary distribution using eigenvalue decomposition\n",
    "    def stationary_dist(self) -> np.array:\n",
    "        e_value, e_vec = np.linalg.eig(self.tran_mat.T)\n",
    "        out = np.array(e_vec[:, np.where(np.abs(e_value- 1.) < 1e-5)[0][0]])\n",
    "        out = out/np.sum(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sunny': 0, 'Cloudy': 1, 'Rainy': 2, 'Windy': 3}\n",
      "[[0.1  0.2  0.3  0.4 ]\n",
      " [0.25 0.25 0.3  0.2 ]\n",
      " [0.1  0.2  0.3  0.4 ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "[0.18027211 0.22675737 0.2845805  0.30839002]\n"
     ]
    }
   ],
   "source": [
    "# Test class\n",
    "Input = {'Sunny': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Windy': 0.4},\n",
    "         'Cloudy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.3, 'Windy': 0.2},\n",
    "         'Rainy': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Windy': 0.4},\n",
    "         'Windy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.25, 'Windy': 0.25}}\n",
    "test_MP = MP(Input)\n",
    "print(test_MP.get_states())\n",
    "print(test_MP.get_tran_mat())\n",
    "print(test_MP.stationary_dist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class design for MRP\n",
    "- Separately implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP\n",
    "- Write code to convert/cast the $r(s,s')$ definition of MRP to the $R(s)$ definition of MRP (put some thought into code design here)<br>\n",
    "\n",
    "**Given** $v(s) = \\mathbb{E}[G_{t}|S_t = s]= \\mathbb{E}[\\sum^{\\infty}_{i=0}\\gamma^{i} R_{t+i+1}|S_t = s]$ <br>\n",
    "**For state vector $S \\in \\mathbb{R}^d$** $v(S) = \\mathbb{E}[G_{t}|S_t] = \\mathbb{E}[\\sum^{\\infty}_{i=0}\\gamma^{i} P^i R_{t+1}|S_t] = \\mathbb{E}[(I-\\gamma P)^{-1}R_{t+1}|S_t]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert reward helper function\n",
    "def convert_reward(_2nd_def_reward: dict, tran_mat: np.ndarray, state: dict) -> dict:\n",
    "    reward_mat = np.zeros((len(state),len(state)))\n",
    "    # Create reward matrix\n",
    "    for i, row in _2nd_def_reward.items():\n",
    "        for j, reward in row.items():\n",
    "            ind_row = state[i]\n",
    "            ind_col = state[j]\n",
    "            if ind(reward[ind_row,ind_col],0):\n",
    "                reward[ind_row,ind_col] = reward\n",
    "    # Cast to 1st def reward vector\n",
    "    reward_vec = np.diag(tran_mat.dot(reward_mat.T))\n",
    "    reward_dict = dict(zip(state.keys(),reward_vec))\n",
    "    return reward_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MRP by Graph\n",
    "\"\"\"\n",
    "    E.g.,\n",
    "    Input = {'Sunny': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Cloudy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.3, 'Cloudy': 0.2},\n",
    "             'Rainy': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Windy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.25, 'Cloudy': 0.25}}\n",
    "    state_reward = {'Rain': 1, 'Sunny': 2, 'Cloudy': 3, 'Windy': 4}\n",
    "    gamma = 0.5\n",
    "    Meaning: Today's weather => tmr's weather\n",
    "\"\"\"\n",
    "class MRP(MP):\n",
    "    \n",
    "    # Initiate state with reward and discount\n",
    "    def __init__(self, state_reward: dict, gamma: float) -> None:\n",
    "        if gamma <0 or gamma >1:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            reward_vec = np.zeros(len(self.state))\n",
    "            for key, ind in self.state.items():\n",
    "                reward_vec[ind] = state_reward[key]\n",
    "            self.reward: np.ndarray = reward_vec\n",
    "            self.gamma: float = gamma\n",
    "    \n",
    "    # Compute value function R(s)\n",
    "    def value_func(self) -> float:\n",
    "        return np.linalg.inv(np.identity(len(self.state))-self.gamma*self.tran_mat).dot(self.reward)\n",
    "\n",
    "    # Compute value function r(s,s')\n",
    "    def value_func_2nd(self,_2nd_def_reward) -> float:\n",
    "        reward_dict = convert_reward(_2nd_def_reward)\n",
    "        reward_vec = np.zeros(len(self.state))\n",
    "        for key, ind in self.state.items():\n",
    "            reward_vec[ind] = reward_dict[key]\n",
    "        self.reward = reward_vec\n",
    "        return self.value_func()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
