{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MP/MRP definitions and MRP Value Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Process** = (finite) state set $S$ + transition probability matrix between states following the Markov property: $P(X_{t+1} = s|X_t = s') = P(X_{t+1} = s|X_t = s',...,X_0 = x)$<br>\n",
    "**Markov Reward Process** = Markov Process + Reward associated with each state + discount factor\n",
    "- Reward: $R_s = \\mathbb{E}[R_{t+1}|S_t = s]$ or $R(s,s')$ (two consective states)\n",
    "- Discount factor: $\\gamma$<br>\n",
    "\n",
    "**Return:** $G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum^{\\infty}_{i=t+1}\\gamma^{i-t-1} R_i$ <br>\n",
    "**Value function**: $v(s) = \\mathbb{E}[G_{t}|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class design for MP/MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MP by state set and transition matrix\n",
    "# State set for MP: map (label to index)\n",
    "# Transition matrix: matrix (nparray)\n",
    "\"\"\"\n",
    "    E.g.,\n",
    "    state = {Rain: 0, Sunny: 1, Cloudy: 1, Windy: 1}\n",
    "    tran_mat = np.asarray([0.1,0.2,0.3,0.4,\n",
    "            0.25,0.25,0.25,0.25,\n",
    "            0.1,0.2,0.3,0.4,\n",
    "            0.25,0.25,0.25,0.25]).reshape((4,4))\n",
    "    # Today's weather => tmr's weather\n",
    "\"\"\"\n",
    "class MP:\n",
    "    def __init__(self, state, tran_mat):\n",
    "        self.state = state\n",
    "        self.tran_mat = tran_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MRP by state set and transition matrix\n",
    "# State set for MRP: map (label to index)\n",
    "# State with the 1st reward definition: map (label to reward)\n",
    "# Transition matrix: matrix (nparray)\n",
    "\"\"\"\n",
    "    E.g.,\n",
    "    state = {Rain: 0, Sunny: 1, Cloudy: 1, Windy: 1}\n",
    "    tran_mat = np.asarray([0.1,0.2,0.3,0.4,\n",
    "            0.25,0.25,0.25,0.25,\n",
    "            0.1,0.2,0.3,0.4,\n",
    "            0.25,0.25,0.25,0.25]).reshape((4,4))\n",
    "    state_reward = {Rain: 1, Sunny: 2, Cloudy: 3, Windy: 4}\n",
    "    # Today's weather => tmr's weather\n",
    "\"\"\"\n",
    "class MRP(MP):\n",
    "    def __init__(self, state_reward):\n",
    "        self.reward = state_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward functions and distribution\n",
    "- Separately implement the r(s,s') and the R(s) = \\sum_{s'} p(s,s') * r(s,s') definitions of MRP\n",
    "- Write code to convert/cast the r(s,s') definition of MRP to the R(s) definition of MRP (put some thought into code design here)\n",
    "- Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
